### 关于scrapy的三种翻页方式

在进行scrapy多页爬取数据时，通常需要我们进行翻页爬取数据，下面介绍三种翻页方式：

- start_urls列表
- start_requests()方法
- parse()方法

#### start_urls列表

在我们使用scrapy创建创建爬虫文件时，通常会用到如下命令：

````shell
scrapy genspider 爬虫名称 待爬取网页
````

执行完上述命令后，会在项目路径下的同名项目路径下的spiders文件中生成指定名称的爬虫文件，一般该爬虫文件结构如下：

![image-20221005204546938](D:\学习\大数据\技能大赛\学习笔记\学习笔记截图\scrapy爬虫代码结构.png)

其中：

- name表示爬虫的名称
- allowed_domains表示允许访问的域名(限制爬虫爬取的网站)
- start_urls表示待爬取的网页url列表(具体要爬哪些网页)
- parse()是针对scrapy核心调度器返回的响应的处理方法

最简单的方法就是：将所有待爬取网页的url放入start_urls列表中，在爬虫启动时，会优先检测start_urls中的url，并将其中的url依次封装成scrapy.Request()对象返回给scrapy核心调度器，再由调度器发送给下载器进行请求，下载器将请求得到的响应封装成scrapy.Response()对象返回给调度器，最后由调度器将响应对象返回给爬虫文件中的parse方法进行解析。

例如：

![image-20221006085049100](D:\学习\大数据\技能大赛\学习笔记\学习笔记截图\scrapy爬虫第一种翻页方式示例.png)

#### start_requests()方法

start_requests()方法是scrapy.Spider基类的方法，我们所创建的爬虫类都是继承该基类的，所以我们可以重写基类的方法。start_requests()方法在爬虫启动时，scrapy核心调度器会自动调用该方法，并且只会**调用一次**，该方法会读取start_urls列表中的url，并会依次生成scrapy.Request对象转交给scrapy核心调度器，由核心调度器交由下载器进行网页的下载并返回给parse()方法进行处理。

该方法在创建scrapy.Request对象的过程中，可以指定更高级的请求头。通常我们设置请求头的地方有两个：

- scrapy项目的settings文件中
- start_requests()方法中

上述两种设置请求头的位置不同，优先级也不同。start_requests()方法中设置的请求头优先于scrapy项目的settings文件中设置的请求头，也就是说scrapy项目中的创建的scrapy.Request对象会优先使用start_requests()方法中声明的请求头。

start_requests()方法的主要应用场景：当每一次请求的请求头不一样并且请求头信息和请求url有关系时，可以考虑使用start_requests()方法。

例如：

![image-20221006091415059](D:\学习\大数据\技能大赛\学习笔记\学习笔记截图\scrapy爬虫第二种翻页方式示例.png)

#### parse()方法

parse()方法是爬虫文件的核心方法，主要负责接收下载器返回的scrapy.Response对象并对其进行解析提取数据。使用parse()方法进行翻页的**前提条件**是：我们可以在当前的页面解析出下一页的地址。

例如：

以起点中文网为例(网址：[小说畅销排行榜单_起点小说畅销排行-起点中文网 (qidian.com)](https://www.qidian.com/rank/hotsales/page1/))

![image-20221006092240858](D:\学习\大数据\技能大赛\学习笔记\学习笔记截图\scrapy爬虫第三种翻页方式起点中文网示例.png)

在当前页面可以解析出下一页的地址，因此，若要使用scrapy对这些网页进行爬取可以使用parse()方法对下一页的url进行解析，然后封装成scrapy.Reques对象提交给scrapy核心调度器进行下一步操作。

注意：这里为了保持代码的健壮性，考虑最后一页(注：最后一页没有下一页)的特殊情况，需要进行设定，只有当解析出下一页的url时，再进行提交；否则，不提交。

例如：

![image-20221006093039111](D:\学习\大数据\技能大赛\学习笔记\学习笔记截图\scrapy爬虫第三种翻页方式示例.png)

#### 总结

三种翻页方式的特点不一，使用场景不一，根据需求进行选择。

- start_urls[]列表法是最简单的翻页方法。
- start_requests()方法需要观察出请求头信息与请求地址之间的关系。
- parse()方法需要明确爬取网页是否满足使用条件同时需要一定的网页源码解析能力。